{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_LSTM_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDz_SuP8GwOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "00cbf93c-e3fb-4066-d504-1cac3153c3f2"
      },
      "source": [
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data2')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1s9nJuoaHfKdspaEuM4GVWr7IvIIk2AcL' in parents\"}).GetList()#{'q': \"'1S02HappJ9ZBnkhLYg-PMMxTTd7HWsGCa' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: rider1_ride4_timed10.csv, id: 1pnZ6Jj8rcunHb0ZKnoBR00Fg_kny13G5\n",
            "downloading to /root/data2/rider1_ride4_timed10.csv\n",
            "title: rider1_ride3_timed10.csv, id: 1izLX4U0D3ZtuajBZ0YGEKS9J8xOgEm_p\n",
            "downloading to /root/data2/rider1_ride3_timed10.csv\n",
            "title: rider1_ride2_timed10.csv, id: 1VRG_f8JD6EfJ_-huF7jCLr3zwZuPTEl0\n",
            "downloading to /root/data2/rider1_ride2_timed10.csv\n",
            "title: rider1_ride1_timed10.csv, id: 15wMuHMcG9OC8goI6MSXvpdaaC1tI7Hwg\n",
            "downloading to /root/data2/rider1_ride1_timed10.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOT2TauyG6W8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b1e433a-068e-45d7-bf72-84c9ff055a95"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "import copy\n",
        "#tf.compat.v1.enable_eager_execution() \n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "#read csv -----all data------, rename the old files with _old\n",
        "#all_files = glob.glob(\"//root//data2//*raw.csv\")\n",
        "#test_files = glob.glob(\"//root//data2//*old.csv\")\n",
        "\n",
        "#read csv rider wise, rename the test file as test_ and keep the rider files as is\n",
        "all_files = glob.glob(\"//root//data2//rider*.csv\")\n",
        "test_files = glob.glob(\"//root//data2//test*.csv\")\n",
        "\n",
        "li=[]\n",
        "for filename in all_files:\n",
        "    print('importing...',filename)\n",
        "    df1 = pd.read_csv(filename, index_col=None, header=0)\n",
        "    print('before drop',len(df1))\n",
        "    df1.replace(r\"^\\s*$\",np.nan,inplace=True, regex=True)\n",
        "    df1.replace(r\"^\\s*true$\",1,inplace=True, regex=True)\n",
        "    df1.replace(r\"^\\s*false$\",0,inplace=True, regex=True)\n",
        "    df1.dropna(axis=0,how='any',inplace=True)\n",
        "    print('after drop',len(df1))\n",
        "    li.append(df1)\n",
        "\n",
        "df = pd.concat(li, axis=0, ignore_index=True)\n",
        "total_trn = len(df)\n",
        "print('total_train',len(df))\n",
        "\n",
        "li1=[]\n",
        "for filename in test_files:\n",
        "  print('importing...',filename)\n",
        "  df1=pd.read_csv(filename, index_col=None, header=0)\n",
        "  #df1[['cadence','altitude_smooth','grade_smooth','velocity_smooth','watts']].str.strip()\n",
        "  print('before drop1',len(df1))\n",
        "  df1.replace(r\"^\\s*$\",np.nan,inplace=True, regex=True)\n",
        "  df1.replace(r\"^\\s*true$\",1,inplace=True, regex=True)\n",
        "  df1.replace(r\"^\\s*false$\",0,inplace=True, regex=True)\n",
        "  #df1.replace(\"\",np.NaN,inplace=True)\n",
        "  #print(df1['watts'].head())\n",
        "  df1.dropna(axis=0,how='any',inplace=True)\n",
        "  #df1.to_csv('new3182519785_dropped.csv')\n",
        "  #files.download('new3182519785_dropped.csv')\n",
        "  print('after drop1',len(df1))\n",
        "  li1.append(df1)\n",
        "\n",
        "test_df = pd.concat(li1, axis=0, ignore_index=True)\n",
        "test_df_copy = copy.deepcopy(test_df)\n",
        "#test_df=test_df.dropna()\n",
        "total_tst = len(test_df)\n",
        "print('total_test',total_tst)\n",
        "#print(test_df['watts'].empty())\n",
        "#test_df_filtered = test_df[test_df['watts'].str.strip()!=np.NaN]\n",
        "print(test_df.tail())\n",
        "#df1 = pd.read_csv('_5.csv')\n",
        "#df=df.append(df1, ignore_index=True)\n",
        "\n",
        "print (test_df.isnull().any())\n",
        "print(test_df.columns)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing... //root//data2/rider1_ride1_timed10.csv\n",
            "before drop 1369\n",
            "after drop 1369\n",
            "importing... //root//data2/rider1_ride4_timed10.csv\n",
            "before drop 663\n",
            "after drop 663\n",
            "importing... //root//data2/rider1_ride2_timed10.csv\n",
            "before drop 1330\n",
            "after drop 1330\n",
            "total_train 3362\n",
            "importing... //root//data2/test_rider1_ride3_timed10.csv\n",
            "before drop1 231\n",
            "after drop1 231\n",
            "total_test 231\n",
            "     Unnamed: 0  Unnamed: 0.1  ...  accel_sensor(m2/s)  accel_gps(m2/s)\n",
            "226         226        1138.5  ...            0.004493         0.002644\n",
            "227         227        1143.5  ...            0.003991         0.002848\n",
            "228         228        1148.5  ...            0.005808         0.003444\n",
            "229         229        1153.5  ...            0.008122         0.003439\n",
            "230         230        1158.5  ...            0.007741         0.003458\n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Unnamed: 0                   False\n",
            "Unnamed: 0.1                 False\n",
            "time                         False\n",
            "timer_time                   False\n",
            "distance                     False\n",
            "gap_distance                 False\n",
            "watts                        False\n",
            "watts_calc                   False\n",
            "heartrate                    False\n",
            "cadence                      False\n",
            "velocity_smooth              False\n",
            "pace                         False\n",
            "gap                          False\n",
            "lat                          False\n",
            "lng                          False\n",
            "temp                         False\n",
            "altitude                     False\n",
            "altitude_smooth              False\n",
            "grade_smooth                 False\n",
            "distance_calc(m)             False\n",
            "gps_delta(m)                 False\n",
            "weight_bike_rider_50         False\n",
            "weight_bike_rider_60         False\n",
            "weight_bike_rider_70         False\n",
            "weight_bike_rider_80         False\n",
            "weight_bike_rider_90         False\n",
            "weight_bike_rider_100        False\n",
            "weight_bike_rider_110        False\n",
            "weight_bike_rider_120        False\n",
            "speed_from_sensor(m/s)       False\n",
            "speed_from_gps(m/s)          False\n",
            "wind_resistance_sensor(N)    False\n",
            "wind_resistance_gps(N)       False\n",
            "accel_sensor(m2/s)           False\n",
            "accel_gps(m2/s)              False\n",
            "dtype: bool\n",
            "Index(['Unnamed: 0', 'Unnamed: 0.1', 'time', 'timer_time', 'distance',\n",
            "       'gap_distance', 'watts', 'watts_calc', 'heartrate', 'cadence',\n",
            "       'velocity_smooth', 'pace', 'gap', 'lat', 'lng', 'temp', 'altitude',\n",
            "       'altitude_smooth', 'grade_smooth', 'distance_calc(m)', 'gps_delta(m)',\n",
            "       'weight_bike_rider_50', 'weight_bike_rider_60', 'weight_bike_rider_70',\n",
            "       'weight_bike_rider_80', 'weight_bike_rider_90', 'weight_bike_rider_100',\n",
            "       'weight_bike_rider_110', 'weight_bike_rider_120',\n",
            "       'speed_from_sensor(m/s)', 'speed_from_gps(m/s)',\n",
            "       'wind_resistance_sensor(N)', 'wind_resistance_gps(N)',\n",
            "       'accel_sensor(m2/s)', 'accel_gps(m2/s)'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU9OhrdgHU3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "bf75d700-9f17-472c-e187-cffe79f457ed"
      },
      "source": [
        "#total_train 9770,8458,12962,22035\n",
        "#total_test 2210,1354,2814,4707\n",
        "#total_train 29315,25385,38893,66121\n",
        "#total_test 6631,4064,8444,14124\n",
        "#total_train 2930,,,6608\n",
        "#total_test 663,,,1412\n",
        "EVALUATION_INTERVAL = 5000 # How many records you wish to pass in each epoch interval goes here, range can >0 to <= training data set row count\n",
        "EPOCHS = 30 #how many times do you want to iterate\n",
        "TRAIN_SPLIT =  total_trn \n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE=32\n",
        "\n",
        "#multivariate data\n",
        "\n",
        "#minimal and extended feature headers. if you normalize and then use the data, remove weight from features, because it is a column of constants and will break the model.\n",
        "\n",
        "# for all files, make sure you have the same no. of cols in old and new and in same order\n",
        "\n",
        "header_sensor_minimal_new = ['distance','speed_from_sensor(m/s)','accel_sensor(m2/s)','grade_smooth','watts']#'weight_bike_rider_80',\n",
        "header_gps_minimal_new = ['distance_calc(m)','speed_from_gps(m/s)','accel_gps(m2/s)','grade_smooth','watts']#'weight_bike_rider_80',\n",
        "\n",
        "header_sensor_extended_new = ['distance','speed_from_sensor(m/s)','accel_sensor(m2/s)','grade_smooth','wind_resistance_sensor(N)',\n",
        "                              'temp','cadence','altitude','watts']#'weight_bike_rider_80','heartrate',\n",
        "header_gps_extended_new = ['distance_calc(m)','speed_from_gps(m/s)','accel_gps(m2/s)','grade_smooth','wind_resistance_gps(N)',\n",
        "                           'temp','cadence','altitude','watts']#'weight_bike_rider_80','heartrate',\n",
        "header_sensor_minimal_old = ['distance(m)','speed(m/s)','accel_sensor(m2/s)','grade(%)','watts' ] #,'weight_bike_rider_80'\n",
        "header_gps_minimal_old = ['distance_calc(m)','speed_from_gps(m/s)','accel_gps(m2/s)','grade(%)', 'watts' ]#,'weight_bike_rider_80'\n",
        "\n",
        "header_sensor_extended_old = ['distance(m)','speed(m/s)','accel_sensor(m2/s)','grade(%)','wind_resistance_sensor(N)',\n",
        "                              'temperature(C)','cadence(rpm)','altitude(m)','watts']#'weight_bike_rider_80',\n",
        "header_gps_extended_old = ['distance_calc(m)','speed_from_gps(m/s)','accel_gps(m2/s)','grade(%)','wind_resistance_gps(N)',\n",
        "                           'temperature(C)','cadence(rpm)','altitude(m)','watts']#'weight_bike_rider_80',\n",
        "\n",
        "features_considered = header_sensor_minimal_new # change header here to switch between min and extended, if riderwise then put the same header, if all files put new, then old\n",
        "\n",
        "features = df[features_considered]\n",
        "\n",
        "print(features.head())\n",
        "\n",
        "test_features_considered = header_sensor_minimal_new # change header here to switch between min and extended, if riderwise then put the same header, if all files put new, then old\n",
        "\n",
        "\n",
        "test_features=test_df[test_features_considered]\n",
        "test_features.columns=features_considered \n",
        "\n",
        "print(test_features.tail())\n",
        "\n",
        "frames = [features,test_features] \n",
        "features = pd.concat(frames)  \n",
        "features[features_considered] = features[features_considered].astype(float) \n",
        "\n",
        "# switch on when all files - last fle of rider 4\n",
        "'''\n",
        "index1 = total_trn - 13306  \n",
        "index2 = total_trn + 14204  # switch on when all files - first file of rider 4\n",
        "train1 = features.iloc[:index1,:] # switch on when all files\n",
        "test = features.iloc[index1:index2, :] # switch on when all files\n",
        "train2 = features.iloc[index2:,:]# switch on when all files\n",
        "frames2 = [train1,train2,test] # switch on when all files - used when all data\n",
        "features = pd.concat(frames2)  # switch on when all files - used when all data\n",
        "\n",
        "total_tst = len(test)\n",
        "print(total_tst)\n",
        "'''\n",
        "\n",
        "print(features.head())\n",
        "print(features.tail())\n",
        "print('total dataset',len(features))\n",
        "\n",
        "\n",
        "#print(features[~features.applymap(np.isreal).all(1)])\n",
        "#plotting individual graphs for features\n",
        "#features.plot(subplots=True)\n",
        "#plt.legend(loc=1)\n",
        "#plt.savefig('features_plot.png')\n",
        "#files.download(\"features_plot.png\") \n",
        "\n",
        "#plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     distance  speed_from_sensor(m/s)  ...  grade_smooth       watts\n",
            "0   39.260000                   7.300  ... -1.800000e-01  216.700000\n",
            "1   78.600000                   7.865  ...  1.110223e-17  217.750000\n",
            "2  121.196667                   8.300  ... -3.333333e-02  219.833333\n",
            "3  165.370000                   8.475  ... -2.500000e-03  214.550000\n",
            "4  210.276000                   8.672  ... -1.400000e-01  216.480000\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "         distance  speed_from_sensor(m/s)  ...  grade_smooth       watts\n",
            "226  11617.122952               10.395551  ...      0.282335  265.926872\n",
            "227  11670.002500               10.400614  ...      0.282325  266.105263\n",
            "228  11722.939694               10.409956  ...      0.279083  266.310917\n",
            "229  11775.959870               10.404993  ...      0.275304  266.241304\n",
            "230  11828.983377               10.408348  ...      0.273420  266.387013\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "     distance  speed_from_sensor(m/s)  ...  grade_smooth       watts\n",
            "0   39.260000                   7.300  ... -1.800000e-01  216.700000\n",
            "1   78.600000                   7.865  ...  1.110223e-17  217.750000\n",
            "2  121.196667                   8.300  ... -3.333333e-02  219.833333\n",
            "3  165.370000                   8.475  ... -2.500000e-03  214.550000\n",
            "4  210.276000                   8.672  ... -1.400000e-01  216.480000\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "         distance  speed_from_sensor(m/s)  ...  grade_smooth       watts\n",
            "226  11617.122952               10.395551  ...      0.282335  265.926872\n",
            "227  11670.002500               10.400614  ...      0.282325  266.105263\n",
            "228  11722.939694               10.409956  ...      0.279083  266.310917\n",
            "229  11775.959870               10.404993  ...      0.275304  266.241304\n",
            "230  11828.983377               10.408348  ...      0.273420  266.387013\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "total dataset 3593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zWhl-191h5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6481c18-b1ec-458a-d4ea-bcf3168ebc13"
      },
      "source": [
        "#TRAIN_SPLIT = len(train1)+len(train2) # used when all file\n",
        "#print(TRAIN_SPLIT)\n",
        "\n",
        "dataset = features.values.astype('float')\n",
        "\n",
        "#data normalization\n",
        "\n",
        "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
        "data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
        "#print(data_mean)\n",
        "#print(data_std)\n",
        "dataset = (dataset-data_mean)/data_std\n",
        "\n",
        "#data standardization\n",
        "#min = dataset[:TRAIN_SPLIT].min()\n",
        "#max = dataset[:TRAIN_SPLIT].max()\n",
        "#dataset = (dataset-min)/(max-min)\n",
        "\n",
        "\n",
        "\n",
        "#download normalized dataset\n",
        "#dataset.to_csv('normalized_dataset.csv')\n",
        "#files.download('normalized_dataset.csv')\n",
        "\n",
        "#single step prediction\n",
        "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, step, single_step=False): #dataset, power, 0, 8000, 20, 0, 0, True\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index): # 20, 8000 for val : 8000 to 10000\n",
        "    indices = range(i-history_size, i, step) # 20-20=0, 20, 1 = [0 to 20, with step 1], for val : 7980  to 8000 with step 1\n",
        "    data.append(dataset[indices]) # 20 x 1 D array\n",
        "\n",
        "    if single_step:\n",
        "      labels.append(target[i+target_size]) # 20+0=20 target[20] val: target[8000]\n",
        "    else:\n",
        "      labels.append(target[i:i+target_size])\n",
        "\n",
        "  return np.array(data), np.array(labels)\n",
        "\n",
        "#past window of timesteps\n",
        "past_history = 20\n",
        "future_target = 0\n",
        "STEP = 1\n",
        "\n",
        "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, -1], 0,\n",
        "                                                   TRAIN_SPLIT, past_history,\n",
        "                                                   future_target, STEP,\n",
        "                                                   single_step=True)\n",
        "\n",
        "#validation set (if used) Used for TESTING currently\n",
        "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, -1],\n",
        "                                               TRAIN_SPLIT, None, past_history,\n",
        "                                               future_target, STEP,\n",
        "                                               single_step=True)\n",
        "\n",
        "#train features mapped to corresponding powers \n",
        "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
        "\n",
        "#loading into memory\n",
        "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat() #.shuffle(BUFFER_SIZE).\n",
        "\n",
        "\n",
        "\n",
        "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
        "val_data_single = val_data_single.batch(BATCH_SIZE).repeat() \n",
        "\n",
        "\n",
        "\n",
        "single_step_model = tf.keras.models.Sequential()\n",
        "single_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                           input_shape=x_train_single.shape[-2:]))\n",
        "single_step_model.add(tf.keras.layers.Dense(1))\n",
        "#adam=tf.keras.optimizers.Adam(\n",
        "#    learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "#   name='Adam')#, **kwargs)\n",
        "\n",
        "single_step_model.compile(optimizer='Adam', loss='mae') #tf.keras.optimizers.RMSprop()\n",
        "\n",
        "#fitting\n",
        "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
        "                                            steps_per_epoch=EVALUATION_INTERVAL)\n",
        "                                            #validation_data=val_data_single,  #turn on when want to validate while training\n",
        "                                            #validation_steps=10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0232\n",
            "Epoch 2/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0112\n",
            "Epoch 3/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0096\n",
            "Epoch 4/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0089\n",
            "Epoch 5/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0083\n",
            "Epoch 6/30\n",
            "5000/5000 [==============================] - 29s 6ms/step - loss: 0.0078\n",
            "Epoch 7/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0075\n",
            "Epoch 8/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0073\n",
            "Epoch 9/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0071\n",
            "Epoch 10/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0069\n",
            "Epoch 11/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0067\n",
            "Epoch 12/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0067\n",
            "Epoch 13/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0065\n",
            "Epoch 14/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0065\n",
            "Epoch 15/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0063\n",
            "Epoch 16/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0062\n",
            "Epoch 17/30\n",
            "5000/5000 [==============================] - 29s 6ms/step - loss: 0.0062\n",
            "Epoch 18/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0060\n",
            "Epoch 19/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0060\n",
            "Epoch 20/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0059\n",
            "Epoch 21/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0058\n",
            "Epoch 22/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0058\n",
            "Epoch 23/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0057\n",
            "Epoch 24/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0057\n",
            "Epoch 25/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0056\n",
            "Epoch 26/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0056\n",
            "Epoch 27/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0055\n",
            "Epoch 28/30\n",
            "5000/5000 [==============================] - 29s 6ms/step - loss: 0.0054\n",
            "Epoch 29/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0054\n",
            "Epoch 30/30\n",
            "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC9u3PbvHefe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "38fb4edb-9ff1-41d9-d709-6aa17cfd5112"
      },
      "source": [
        "from sklearn import metrics\n",
        "def plot_train_history(history, title):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "#only turn on if using validation\n",
        "#plot_train_history(single_step_history,'Single Step Training and validation loss')\n",
        "\n",
        "#print('Error RMSE : ', tf.sqrt(tf.reduce_mean((y_val_single - single_step_model.predict(x_val_single))**2)))\n",
        "\n",
        "\n",
        "allrmse=[]\n",
        "allmae=[]\n",
        "c=0\n",
        "yhatlist=[]\n",
        "ylist=[]\n",
        "for x, y in val_data_single.take((total_tst-20)//32):\n",
        "  #print(y.shape)\n",
        "  #print(x.shape)\n",
        "  #print('Error RMSE : ', tf.sqrt(tf.reduce_mean((y[0].numpy() - single_step_model.predict(x)[0])**2)))\n",
        "  #Y=round(y[0].numpy(),3)\n",
        "  #Yhat=round(single_step_model.predict(x)[0][0],3)\n",
        "  #YPower = (Y*data_std[-1])+data_mean[-1]\n",
        "  #Yhatpower = (Yhat*data_std[-1])+data_mean[-1]\n",
        "  \n",
        "  #print('y: ', y[0].numpy(), YPower)\n",
        "  #print('yhat: ', single_step_model.predict(x)[0], Yhatpower)\n",
        "  #plot = show_plot([x[0][:, -1].numpy(), y[0].numpy(), # this can be switched on to plot\n",
        "  #                 single_step_model.predict(x)[0]], 0,\n",
        "  #              'Single Step Prediction',Y,Yhat)\n",
        "  #plt.savefig('Lstm_'+str(c)+'.png')\n",
        "  #files.download('Lstm_'+str(c)+'.png')\n",
        "  c+=1\n",
        "  print(c)\n",
        "  #plot.show()\n",
        "  rmse=[]\n",
        "  mae =[]\n",
        "\n",
        "  for j in range(0,len(y)):\n",
        "    '''ylist.append(((y[j].numpy()*data_std[-1])+data_mean[-1]) if ((y[j].numpy()*data_std[-1])+data_mean[-1])>0 else 0)\n",
        "    yhatlist.append(((single_step_model.predict(x)[j][0]*data_std[-1])+data_mean[-1]) if ((single_step_model.predict(x)[j][0]*data_std[-1])+data_mean[-1])>0 else 0 )\n",
        "    rmse.append(tf.sqrt(tf.reduce_mean((y[j].numpy() - single_step_model.predict(x)[j])**2)))#.numpy())'''\n",
        "    ylist.append((y[j].numpy()*data_std[-1])+data_mean[-1])\n",
        "    #ylist.append(y[j].numpy())\n",
        "    #print(y[j].numpy(),single_step_model.predict(x)[j],(y[j].numpy() - (single_step_model.predict(x)[j])))\n",
        "    yhatlist.append((single_step_model.predict(x)[j][0]*data_std[-1])+data_mean[-1])\n",
        "    #yhatlist.append(single_step_model.predict(x)[j][0])\n",
        "    #rmse.append(tf.sqrt(tf.reduce_mean((y[j].numpy() - single_step_model.predict(x)[j][0])**2)))#.numpy())\n",
        "    #rmse.append(metrics.mean_squared_error(y[j].numpy(),single_step_model.predict(x)[j][0],squared=False))\n",
        "    #mae.append(metrics.mean_absolute_error(y[j].numpy(),single_step_model.predict(x)[j][0]))\n",
        "\n",
        "  #print(rmse)\n",
        "  #print(np.mean(rmse))\n",
        "  #allrmse.append(np.mean(rmse))\n",
        "  #allmae.append(np.mean(mae))\n",
        "print('RMSE: ',metrics.mean_squared_error(ylist,yhatlist,squared=False))\n",
        "print('MAE: ',metrics.mean_absolute_error(ylist,yhatlist))\n",
        "print('Mean y_pred_lstm', np.mean(yhatlist))\n",
        "print('MEan ytest', np.mean(ylist))\n",
        "df2=pd.DataFrame({'ytest':ylist, 'yhat':yhatlist})\n",
        "#df2.to_csv('pred_lstm.csv')\n",
        "#print(len(all))\n",
        "#single_step_model.save('rider4_timed3_header_sensor_minimal_old.h5')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "RMSE:  18.517193054387814\n",
            "MAE:  18.26155544105106\n",
            "Mean y_pred_lstm 243.6575664216551\n",
            "MEan ytest 261.91912186270616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxmdHpTGO1HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "379a3f9d-1b07-4121-a1db-ef2cc9967bfc"
      },
      "source": [
        "df2.to_csv('pred_lstm.csv')\n",
        "files.download('pred_lstm.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d7488e526c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pred_lstm.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    }
  ]
}